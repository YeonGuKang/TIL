Doker - 컨테이너 기반의 가상화 시스템 (동일한 실습환경 설정 가능)

1차원 - vector\
2차원 - matrix\
3차원 - tensor

NLP - 3차원 tensor를 이용해서 처리

dim - 하나의 문장\
여러개의 dim(문장)들이 batch size로 쌓여있다.

훈련 데이터의 개수가 굉장히 많을 때, 컴퓨터가 한 번에 들고가서 처리할 양을 배치 크기(batch size)라고 합니다.

행렬에서는 첫번째 차원 = 행

long = int\
float = 실수

print(t[:, 1]) - 첫번째 차원은 모두 나타내고 두번째 차원은 1 index만 표현해라

dim=0 -> 첫번째 차원을 제거해라 즉, 행렬에서 '열'만 남겨라\
dim=1 -> 두번째 차원을 제거해라 즉, 행렬에서 '행'만 남겨라

-1은 인덱스상에서 마지막 차원을 의미\
X = X.view(-1, 28*28) 하지만 이것은 앞에 차원은 모르겠고 \
뒤에 차원 element 28 * 28에 맞춰서 element를 구성해라

[[0], [1], [2]] = 3 X 1

[[0., 1., 2.]] = 1 X 3

[] = 1차원\
[[]] = 2차원\
[[[]]] = 3차원

[[[0, 1, 2],
               [3, 4, 5]],
              [[6, 7, 8],
               [9, 10, 11]]]

2 X 2 X 3

[[]]가 2개씩 있고 , [[]]안에 []가 2개씩 있으며 , []안에 각 원소는 3개씩 있다~

print(ft.view([-1, 3])) - 앞에 차원은 모르겠고, 두번째 차원을 3개의 element를 가지도록 해라\
2 X 2 X 3 -> 4 X 3 이 된다. (뒤에 3을 맞추기위해 앞이 4로 합쳐짐) \
곱의 결과는 element의 합 = 항상 같아야함\
따라서 print(ft.view([-1, 1, 3])) = 4 X 1 X 3 이된다.

Squeeze는 view를 이용해서 element가 1인 차원을 삭제해줌\
print(ft.squeeze()) 를 3 X 1에 사용하면 -> (3,) 으로 바뀜\
ft.squeeze(dim=x) = x 차원에 element가 1개이면 해당 차원을 없앰

unsqueeze = 내가 원하는 dimension에 element 1 로 해당 차원을 만들어냄\
print(ft.unsqueeze(0)) == print(ft.view(1, -1))

### 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.
W = torch.zeros(1, requires_grad=True) 

 'SGD'는 경사 하강법의 일종입니다. lr은 학습률(learning rate)를 의미합니다.

### 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차
### 앞서 배운 torch.mean으로 평균을 구한다.
cost = torch.mean((hypothesis - y_train) ** 2) 

cost 는 loss (얼마나 우리가 원하는 값과 가까운가?)

optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화합니다.\
 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다.\
 그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됩니다. \
그 다음 경사 하강법 최적화 함수 opimizer의 .step() 함수를 호출하여 인수로 들어갔던\
 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트합니다.

H(x) 식에 입력 x로부터 예측된 y를 얻는 것을 forward 연산이라고 합니다.

### gradient == 기울기
### gradient를 0으로 초기화
#optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화합니다. 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다
### 파이토치는 미분(backward())을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다.
optimizer.zero_grad() 
### 비용 함수를 미분하여 gradient 계산 (역전파 계산)
cost.backward() 
### 역전파 계산한 값으로 W와 b를 업데이트
optimizer.step() 

https://wikidocs.net/52460

https://wikidocs.net/55409 - nn.model 이용법


파이토치의 대부분의 구현체들은 대부분 모델을 생성할 때 클래스(Class)를 사용하고 있습니다. 앞서 배운 선형 회귀를 클래스로 구현해보겠습니다. \
앞서 구현한 코드와 다른 점은 오직 클래스로 모델을 구현했다는 점입니다.\
https://wikidocs.net/60036 - model을 클래스로 구현 (필수)

class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스\
    def __init__(self): #\
        super().__init__()\
        self.linear = nn.Linear(1, 1) # 단순 선형 회귀이므로 input_dim=1, output_dim=1.

    def forward(self, x):\
        return self.linear(x)\
model = LinearRegressionModel()


mini batch gradient descent - 전체 데이터가 너무 크므로 이를 균일하게 나누어서\
각각 하나하나 cost를 계산해서  gradient descent를 계산한다.\
전체데이터로 학습하지 않으므로 매끄럽지 않고 거칠게 학습된다.

전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법을 '배치 경사 하강법'이라고 부릅니다. \
반면, 미니 배치 단위로 경사 하강법을 수행하는 방법을 '미니 배치 경사 하강법'이라고 부릅니다.

배치 경사 하강법은 경사 하강법을 할 때, 전체 데이터를 사용하므로 가중치 값이 최적값에 수렴하는 과정이 매우 안정적이지만, 계산량이 너무 많이 듭니다. \
미니 배치 경사 하강법은 경사 하강법을 할 때, 전체 데이터의 일부만을 보고 수행하므로 최적값으로 수렴하는 과정에서 값이 조금 헤매기도 하지만 훈련 속도가 빠릅니다.

배치 크기는 보통 2의 제곱수를 사용합니다. ex) 2, 4, 8, 16, 32, 64... 그 이유는 CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 경우에 데이터 송수신의 효율을 높일 수 있다고 합니다.

둘 중 하나를 결정하는 문제를 이진 분류(Binary Classification)라고 합니다. 그리고 이진 분류를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)가 있습니다.

로지스틱 회귀는 알고리즘의 이름은 회귀이지만 실제로는 분류(Classification) 작업에 사용할 수 있습니다.\
따라서 로지스틱 함수는 0~1의 값을 가지며 이는 이진분류의 확률을 나타낸다!

(s 는 점수) (점수란 '확률'이 되기 이전의 값, softmax를 거치면 진정한 확률이 됨)\
x -> Affine -> sigmoid -> Affine -> s

현재 x_train은 6 × 2의 크기(shape)를 가지는 행렬이며, y_train은 6 × 1의 크기를 가지는 벡터입니다. x_train을 X라고 하고, \
이와 곱해지는 가중치 벡터를 W라고 하였을 때, XW가 성립되기 위해서는 W 벡터의 크기는 2 × 1이어야 합니다. \
그래야 결과 Y가 6 X 1이 나오기 때문!

원-핫 인코딩 - 각각 선택지가 가지는 인덱스를 1로하고 나머지는 0으로 하는 방식\
ex) 	강아지 = [1, 0, 0]\
	고양이 = [0, 1, 0]\
	냉장고 = [0, 0, 1]

만약 강아지 =1 , 고양이 =2, 냉장고=3 이런식으로 나타낸다면 오차를 구할때 제곱으로 구하기 때문에\
컴퓨터는 강아지가 냉장고보다 고양이에 가깝다고 오해를 불러일으킴. 따라서 원-핫 인코딩을 사용함.

softmax - 말 그대로 max 수치를 soft하게 표현하는것\
ex ) 1,2,3 이 있을때 수치로써 0.01 0.02 0.97 이렇게 표현되며 높은 수치가 max값 이다. (확률을 나타냄!)\
항상 합은 1

softmax 회귀는 크로스 엔트로피와 함께 확률로 손실을 나타내는데 사용됨!

결국 소프트맥스 회귀는 선택지의 개수만큼의 차원을 가지는 벡터를 만들고, \
해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 어떤 함수를 지나게 만들어야 합니다. \
위의 그림은 붓꽃 품종 분류하기 문제 등과 같이 선택지의 개수가 3개일때, \
3차원 벡터가 어떤 함수 ?를 지나 원소의 총 합이 1이 되도록 원소들의 값이 변환되는 모습을 보여줍니다.

x -> Affine -> sigmoid -> Affine -> Softmax -> p


엔트로피는 불확실성의 척도입니다. \
정보이론에서의 엔트로피는 불확실성을 나타내며, 엔트로피가 높다는 것은 정보가 많고, 확률이 낮다는 것을 의미합니다.

크로스 엔트로피는 실제 분포 q에 대하여 알지 못하는 상태에서, 모델링을 통하여 구한 분포인 \
p를 통하여 q를 예측하는 것입니다. \
q와 p가 모두 들어가서 크로스 엔트로피라고 한다고 합니다.

소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.\
소프트맥스에서 나온 확률과 정답레이블 T가 크로스 엔트로피 함수로 들어가서 손실 loss가 출력된다!\
정답레이블 T는 원핫벡터로 이루어짐! (정답만 1이고 나머지는 0)

(L은 loss 손실)\
x -> Affine -> sigmoid -> Affine -> Softmax -> Cross entropy error -> L\
보통 위의 식을 합쳐서 아래처럼 나타낸다.

x -> Affine -> sigmoid -> Affine -> Softmax with Loss -> L\
신경망 학습에서 계산 그래프의 최종 출력은 손실하며, 그 값은 스칼라이다!

https://wikidocs.net/59427 - 소프트맥스 회귀

하이퍼파라미터와 매개변수의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수라는 점입니다. \
선형 회귀 챕터에서 배우게 되는 경사 하강법에서 학습률(learning rate)이 이에 해당되며 딥 러닝에서는 은닉층의 수, 뉴런의 수, 드롭아웃 비율 등이 이에 해당됩니다. \
반면 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값입니다. \
정리하면 하이퍼파라미터는 사람이 정하는 변수인 반면, 매개변수는 기계가 훈련을 통해서 바꾸는 변수라고 할 수 있다.

비유하자면 훈련 데이터는 문제지, 검증 데이터는 모의고사, 테스트 데이터는 실력을 최종적으로 평가하는 수능 시험이라고 볼 수 있습니다.

강화학습은 리워드를 줌으로써 행동을 학습함\
ex) 강아지 훈련

기울기 소실과 폭주를 막기 위해 사용 되는 것들.

1.  활성화 함수를 다른것을 사용\
RELU - 기울기 손실을 막아줌 \
Leaky relu - 음수로 가도 기울기가 0이 아님

2. 가중치 초기화에 신경

weight initialization - 가중치 초기화가 아주 중요하다. 따라서 가중치 초기화 방법도 따로 존재한다.\
(RBM, Xavier, He)\
Xavier , He - Normal과 uniform 분포로 초기화를 시킴! \
Xavier는 S자 형태의 시그모이드, 하이퍼볼릭 탄젠트 함수와 쓰임\
He는 ReLU와 ReLu의 변형 함수와 쓰임

3. 배치 정규화\
배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만듭니다.

배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있습니다. \
내부 공변량 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말합니다. \
이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생합니다. \
배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장합니다.\
결론은 계속해서 모든 층의 분포가 달라지기 때문에 불안정성이 증가한다고 볼 수 있다.

따라서 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행됩니다.\
이렇게 각 층에 들어가는 입력값이 정규화해서 들어가므로 안정성이 크게 증가합니다.



torch.nn = neural network 의 약자\
forward - 순전파\
backward - 역전파

오버피팅을 해결하기 위한 해결책 - Dropout\
Dropout - 무작위로 노드를 껐다 켰다 하면서 동작\
가중치 학습이 일륜화로 되는게 아니라 무작위로 선택돼서 학습이됨 - 계속해서 학습이 바뀌면서 진행됨\
그러면 한가지 test set에 대해서 overfitting이 일어나기 어렵다.\
학습에 대해서는 dropout을 사용해주어야 하지만 \
evaluation 에 대해서는 모든 노드들을 사용해야하므로 정확히 train과 eval을 명시 해 주어야한다.

손실(오차)함수 속에 활성화함수가 들어가 있는 구조!

합성곱 신경망(CNN)-\
1차원으로 변환된 결과는 사람이 보기에도 이게 원래 어떤 이미지였는지 알아보기가 어렵습니다.\
 이는 기계도 마찬가지 입니다. 위와 같이 결과는 변환 전에 가지고 있던 공간적인 구조(spatial structure) 정보가 유실된 상태입니다. \
여기서 공간적인 구조 정보라는 것은 거리가 가까운 어떤 픽셀들끼리는 어떤 연관이 있고, 어떤 픽셀들끼리는 값이 비슷하거나 등을 포함하고 있습니다.\
 결국 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고, 이를 위해 사용하는 것이 합성곱 신경망입니다.

CNN 은 3차원 정보 데이터를 conv와 relu 그리고 pooling을 이용하여서 중요한 특징들을 뽑아\
다시 2차원 정보로 만든 뒤에 그 정보를 앞에서 배운 softmax와 cross entropy로 확인하는 것이다.

우리가 사용하던 완전연결 신경망은 데이터의 형상이 무시된다는 문제점이 존재했다.\
MNIST로 예를들면 우리는 28X28 의 픽셀값을 한줄로 쫙 나열해서 1X28X28로 사용였다.\
이는 공간의 차원을 무시하게 된다. 따라서 공간적 정보를 담기 위해 CNN을 사용한다.\
예를 들면 이미지의 경우에는 세로 X 가로 X 채널(색상)으로 색상이라는 공간적 정보가 중요하다.\
CNN의 필터(커널)의 매개변수가 우리가 그동안에 사용하던 가중치이다!

패딩 - 데이터 주변을 일정값으로 채움 : 주로 출력크기가 줄어드는 문제를 해결하기 위해서 사용한다.\
(그냥 CNN을 사용하면 입력에 비해 출력이 크기가 줄어들기 때문)\
스트라이드 - 필터를 적용하는 위치의 간격

 합성곱 연산을 수행할 때, \
입력 데이터의 채널 수와 필터의 채널수가 같아야 한다.\
즉 필터의 채널은 이미지의 RGB ! 합성곱 연산은 출력의 shape(?by?)를 결정!\
이 shape를 계산할 때에 입력데이터의 채널 수와 필터(RGB)의 채널수가 같아야 한다!

nn.Conv2d(3, 32, 3, padding=1)\
첫번째 parameter 인 3은 input_channel_size가 되겠습니다. \
여기서 input_channel_size는 Input Image의 RGB depth 인 3이 되겠습니다.\
(RGB 는 여기서 필터의 채널 수 -> 입력 채널수와 필터의 채널수가 같다!)

두번째 parameter 인 32는 output_channel_size입니다. \
즉, conv1 layer를 거쳐 몇장의 필터를 만들어 내고 싶은가? 입니다. \
32장의 필터를 만들어 내고 싶으므로, 32가 되겠습니다.\
즉 우리가 RGB를 어떻게 조합해서 서로 다른 필터 32개를 만들어 내고,\
이는 이 필터들을 통과하는 출력 32장이 나온다는 소리다.\
RGB 필터 채널을 묶어서 새로운것을 몇개 만들것이냐 ? 32개를 만들것이다!


https://excelsior-cjh.tistory.com/180
