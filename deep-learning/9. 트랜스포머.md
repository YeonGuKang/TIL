
![image](https://user-images.githubusercontent.com/37290818/116801886-016fd280-ab49-11eb-9e12-2adc7abe4526.png)
RNN을 제거하고 행렬곱으로 한번에 attention 특징을 추출함. 순차적으로 계산하는것이 아닌 한번에 행렬곱으로 계산해서 훨씬 빠르다.\
한번의 연산으로 각 단어의 중요정보를 뽑아서 인코딩 하게된다.

하지만 우리가 RNN을 사용하는 이유가 순서정보를 담기 위해서 인데 트랜스포머는 그럼 어떻게 순서정보를 처리할까?\
정답은 상대적인 포지션 정보를 담고있는 positional encoding에 있다.
![image](https://user-images.githubusercontent.com/37290818/116801968-e2be0b80-ab49-11eb-9004-6a766d2ccefd.png)
인코더와 디코더에 포지션 정보를 넣어서 처리하는 것이다.
