# Word2Vec
word2vec은 단어 임베딩 모델들 중 대표적인 모델이다. 이 글에서는 단어 임베딩 모델의 기본 아이디어와 word2vec의 작동 원리에 대해 알아본다.
또한 dense representation임

dense representation의 장점:

첫번째, dense representation은 적은 차원으로 대상을 표현할 수 있다는 장점이 있다. sparse representation으로 대상을 표현하면 보통 차원 수가 엄청나게 높아진다. 일상적인 텍스트에서 쓰이는 단어의 개수는 몇 천개에 이른다. 이 단어들을 sparse representation으로 표현하려면 몇 천 차원이 필요하다. 게다가 이렇게 만들어진 벡터들은 대부분의 값이 0을 갖는다.

입력 데이터의 차원이 높으면 차원의 저주(curse of dimensionality)라는 문제가 생긴다. 입력 데이터에 0이 너무 많으면 데이터에서 정보를 뽑아내기 어려워진다. 따라서 sparse representation을 쓰면 모델의 학습이 어렵고 성능이 떨어지기 쉽다.

Dense representation으로 단어를 표현할 때는 보통 20 ~ 200차원 정도를 사용한다. Sparse representation에서 몇 천 차원이 필요했던 것에 비해 훨씬 적은 차원이다. 게다가 0이 거의 없고 각각의 차원들이 모두 정보를 들고 있으므로 모델이 더 작동하기 쉬워지는 것이다.

두번째, dense representation은 더 큰 일반화 능력(generalization power)을 갖고 있다. 예를 들어 ‘강아지’라는 단어가 우리가 가진 학습 데이터셋에 자주 나왔고 ‘멍멍이’라는 단어는 별로 나오지 않았다고 생각해보자. sparse representation에는 ‘강아지’와 ‘멍멍이’ 간의 관계가 전혀 표현되지 않는다. 그 때문에 모델이 ‘강아지’에 대해 잘 알게 되더라도 ‘멍멍이’에 대해 더 잘 알게 되는 것은 아니다. 모델이 ‘강아지’가 ‘개’의 아기 상태라는 것을 알게 되었더라도, ‘멍멍이’가 ‘개’와 어떤 관계인지는 여전히 모르는 것이다.

그러나 dense representation에서 ‘강아지’와 ‘멍멍이’가 서로 비슷한 벡터로 표현이 된다면, ‘강아지’에 대한 정보가 ‘멍멍이’에도 일반화될 수 있다. 예컨대 ‘강아지’라는 단어를 입력으로 받고 ‘애완동물’이라는 출력을 하도록 모델이 학습이 된다면, ‘멍멍이’도 비슷한 입력이기 때문에 비슷한 출력이 나올 가능성이 높다. 즉, ‘강아지’라는 단어에 대해 배운 지식을 ‘멍멍이’라는 단어에도 적용할 수 있는 것이다.


Word2Vec에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있습니다. CBOW는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법입니다. 반대로, Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법입니다.
