1. 훈련 데이터에 대한 이해
앞으로 배우게 될 텍스트 분류 작업은 지도 학습(Supervised Learning)에 속합니다. 지도 학습의 훈련 데이터는 레이블이라는 이름의 미리 정답이 적혀있는 데이터로 구성되어 있습니다. 
## 쉽게 비유하면, 기계는 정답이 적혀져 있는 문제지를 열심히 공부하고, 향후에 정답이 없는 문제에 대해서도 정답을 예측해서 대답하게 되는 메커니즘입니다.


seq2seq는 훈련 과정과 테스트 과정(또는 실제 번역기를 사람이 쓸 때)의 작동 방식이 조금 다릅니다. 훈련 과정에서는 디코더에게 인코더가 보낸 컨텍스트 벡터와 실제 정답인 상황인 <sos> je suis étudiant를 입력 받았을 때, je suis étudiant <eos>가 나와야 된다고 정답을 알려주면서 훈련합니다. 이에 대해서는 뒤에 교사 강요(teacher forcing)를 설명하면서 다시 언급하겠습니다. 반면 테스트 과정에서는 앞서 설명한 과정과 같이 디코더는 오직 컨텍스트 벡터와 <sos>만을 입력으로 받은 후에 다음에 올 단어를 예측하고, 그 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복합니다. 즉, 앞서 설명한 과정과 위의 그림은 테스트 과정에 해당됩니다. 이번에는 입, 출력에 쓰이는 단어 토큰들이 있는 부분을 좀 더 확대해보겠습니다.
 
 seq2seq는 인코딩하는 과정에서 임의 길이의 문장을 고정 길이 벡터로 변환하는 작업이있다.\
 해당 인코딩에서 나온 은닉계층의 정보는 decoder에 필요한 정보가 조밀하게 응축되어있다!


앞서 배운 seq2seq 모델은 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들어냈습니다. \
 eos는 문장의 시작과 끝을 알려준다.

하지만 이러한 RNN에 기반한 seq2seq 모델에는 크게 두 가지 문제가 있습니다.
첫째, 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생합니다.
둘째, RNN의 고질적인 문제인 기울기 소실(Vanishing Gradient) 문제가 존재합니다.

즉, 결국 이는 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타났습니다. 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위한 등장한 기법인 어텐션(attention)을 소개합니다.


# 어텐션(Attention)
어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됩니다.

seq2seq에서 정보는 마지막 은닉상태만을 Decoder에 전달하였다. 이점을 개선하고자 모든 은닉계층의 정보를 Decoder에 전달한다.\
이러면 Decoder에 들어가는 벡터의 크기는 입력의 크기와 동일하게 되므로 정보 손실이 발생하지 않게 된다.\
예를 들어 5개의 단어가 입력됐다고 하면 입력수와 같은 5개의 5벡터를 출력하게 됩니다. (이전 encoder는 고정된 크기의 벡터로 출력하였다)\
그런데 여기서 중요한것은 decoder의 첫번 째 계층에는 사실 이전과 같이 encoder의 마지막 줄만 전달한다. 그렇다면 무엇이 다른것인가?\
우리는 위에서 각 단어별로 나온 은닉계층 정보를 모두 벡터로 모아 두었다 그리고 마지막 줄은 decoder에 전달하였다. 나머지 줄은 모두 새로운 계산을 하는 decoder에 계층에 전달한다.\
새로운 계산이란 도착한 모든 벡터중에 필요한 정보에만 주목하여 시계열 변환을 수행하는 계산이다. 이 과정이 핵심이며 이게 바로 어텐션이다.\
즉, decoder에 입력된 입력된 단어와 대응하는 가장 가중치가 높은 단어의 은닉계층 정보(encoder에서 넘어온)에 집중하여 결과를 내보내겠다는 소리이다.\
정리하자면 원래는 encoder에서 나온 은닉계층의 마지막 정보를 이용하지 않고, 들어온 모든 단어의 정보를 이용한다. 하지만 정보를 이용할때에\
decoder에 들어온 입력에 대응하는 가중치가 높은 단어를 계산해서 이를 이용해 결과를 출력한다. 이것이 바로 어텐션의 핵심 이론!

우리가 문장을 번역할 때 머리 속에서 어떤 일이 일어나나요? I=나, cat=고양이와 같이 어떤 단어에 주목(attention)하여 번역합니다. 이와 같이 단어의 대응 관계를 나타내는 정보를 얼라인먼트(alignment)라고 합니다. 지금까지는 얼라인먼트를 수작업으로 만들었습니다. 하지만 어텐션 메커니즘을 활용하면 얼라인먼트를 자동으로 만들 수 있습니다. 

한국어를 영어로 번역한다고 가정했을때, '특정 영어 단어'와 대응 관계가 있는 '특정 한국어 단어' 정보를 골라내는 것이 어텐션의 핵심 포인트입니다. 다시 말해, 필요한 정보에 주목하여 그 정보로부터 문장 변환하는 것입니다.

<img src=https://user-images.githubusercontent.com/37290818/116643395-f03d8f00-a9ab-11eb-913f-4b90da5c0cbd.png>
<img src=https://user-images.githubusercontent.com/37290818/116643473-1e22d380-a9ac-11eb-874e-b92fb22379fc.png>


어텐션 참고 - https://bkshin.tistory.com/entry/NLP-14-%EC%96%B4%ED%85%90%EC%85%98Attention

<img src=https://user-images.githubusercontent.com/37290818/116801775-c15c2000-ab47-11eb-8890-d6a11bc38e6c.png>
h1,h2,h3가 계속해서 쓰이는 것을 알 수있다. 이에따라 정보 소실 문제를 없앤것!
<img src=https://user-images.githubusercontent.com/37290818/116801792-e05ab200-ab47-11eb-8812-b69983cf54ea.png>


https://www.youtube.com/watch?v=WsQLdu2JMgI
 
 
 
# 어텐션(Attention) 보충
 
 key,value,Query를 이용해서 dictionary 자료구조와 비슷함
 ![image](https://user-images.githubusercontent.com/37290818/147190890-f12f8a6a-4e2b-4f1b-acea-32a401287d96.png)

query와 key를 비교해서 유사도가 나오고\
 해당 유사도와 value값을 곱해서 결과를 return함\
 그리고 해당 결과들을 모두 sum해서 결과를 도출함.
![image](https://user-images.githubusercontent.com/37290818/147190958-0949a936-1fa1-4bfb-9894-6e47e624cb07.png)
 
 위의 과정이 Attention 메카니즘과 아주 유사함!!!
 ![image](https://user-images.githubusercontent.com/37290818/147191436-2b274969-8cdd-42b9-a44f-3919d65e9f68.png)

