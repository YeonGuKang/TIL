# 1일차 - 신경망에 대해서 알아보다.


컴퓨터는 단순히 연산을 빨리해주는 큰 기계일 뿐인데, 이를 단순한 연산을 넘어서서\
복잡한 업무를 자동화시키기 위해서 인공지능이 발달하기 시작했다. ex) 사진분류, 세포구분, 체스, 바둑 등등

컴퓨터는 쉬우나 인간이 어려운 작업 - 수백만개의 연산\
인간은 쉬우나 컴퓨터는 어려운 작업 - 사진속에서 사람을 인식하는 작업

하지만 곧 인공지능의 한계가보이면서 점점 포기하기 시작했다.\
그러다 꿀벌이나 비둘기처럼 매우 단순한 뇌구조를 가진 생물이 복잡한 일을 한다는 개념에서\
뇌를 흉내내는 상상을 하였고 이를 실천한다.\
이처럼 생물학으로부터 영감을 받아 등장한것이 바로 '신경망' 인것이다.

### 예측자 - 입력 -> 연산 -> 출력에 있어서 연산의 매개변수값을 계속해서 예측해 나가는 과정
이 과정은 예측 -> 오차확인 -> 오차에 맞춰 다시 예측 -> 오차확인 .... 이런식으로 계속해서 오차에 맞춰 예측을 함으로써\
연산의 값을 도출해 내는것이다.\
정리하자면 오차에 기초해 매개변수 값을 조정해 나가는 것.

### 분류자 - 그룹을 특징에 맞게 그래프에 분류했다고 가정하자. 그러면 특징에 맞게 그룹별로 모여있을 것이다.
이제 이 그래프에 기울기(매개변수)를 조정하며 선을 그어보는데 만약 그 선이 정확하게 그룹들을 분류한다면 그 선은 분류자가 된다.\
예를 들면 선 위에는 무당벌레 , 아래는 귀뚜라미면 우리가 모르는 곤충이 들어왔을 때 선 위에 특징에 존재한다면 분류자에 의해\
그 곤충이 무당벌레라고 우리는 추측할 수 있는것이다.\
하지만 분류자 하나만으로 구분한다는 것은 큰 한계가 존재한다. 따라서 분류자를 여러개 사용하여서 거기서 또 규칙을 찾는것이 중요하다.

### 학습률 - 우리는 선형분류자의 오차와 기울기 매개변수 간의 관계를 이해하고 이를 이용하여 기울기의 조정값을 알 수 있다.
하지만 이는 이전 학습 데이트를 무시하고 최종 학습 데이터에만 맞춰 업데이트 된다. 즉, 단일 학습 데이터가 학습에 지배적인 영향을 준다는 것이다.\
이를 해결하기 위해 학습률을 도입해 조정값을 조정해준다.\
현실 데이터는 잡음이 있거나 오차가 있기때문에 학습률을 이용해서 이런 오류의 영향을 제한할 수 있다.

### 뉴런 - 뉴런은 입력에 즉각 출력이 나오지 않고, 입력 값이 어떤 분계점에 도달해야 출력이 발생한다.
입력신호를 받아 특정 분계정을 넘어서는 경우에 출력신호를 생성해주는 함수를 '활성화 함수'라고 한다.\
이 중 우리는 '시그모이드 함수'를 주로 이용할 것이며 이는 '로지스틱 함수'라고 부르기도 한다.\
시그모이드 함수는 부드러운 s자를 그리며 분계점에 도달하는 함수이다.\
입력이 들어오면 뉴런 1개가 아니라 여러개의 뉴런으로 부터 신호를 전달하고 전달 받는다. 마치 복잡한 링크드-리스트를 생각하면 된다.\
각각의 노드의 연결에는 연결의 가중치로 강도가 존재하며, 낮은 가중치는 신호를 약화시키고 높은 가중치는 신호를 강화한다.

### 행렬 - 행렬은 수많은 뉴런을 만들어 내는데 핵심적인 역할을 한다.
행렬 곱의 성질을 이용하여 입력과 가중치의 곱을 계산해서 시그모이드 함수에 넣으면\
우리는 아주 큰 뉴런을 손쉽게 계산해서 출력을 뽑아낼 수 있다. 행렬의 정말 강력한 기능이다!!

### 오차 - 우리는 앞에서 예측 값과 실제 값의 차이를 오차라고 했다.
이 오차를 줄이기 위해 우리는 오차에 기반을 두고 분류자를 정교화하는 작업을 하였다.\
하지만 위에서 뉴런방식으로 이어져서 나온 결과값이라면 어떻게 정교화 작업을 해야할까?\
결과값에 수많은 노드들이 영향을 미치기 때문에 우리는 모든 노드들에 대해서 정교화 작업을 거쳐야한다.\
여기에 또 가중치가 사용되는데, 가중치가 높을 수록 큰 영향을 준 노드기 때문에 그 노드에 대해서 오차 가중치를 높이는식으로 정교화를 진행한다.\
또한 각 계층에 대해서 출력 계층이 아닌곳에서의 오차는 출력 계층부터 계산되어서 역전파된 오차 * 가중치의 비율의 합으로 계산한다.\
예를 들면 hidden 계층의 node1이 출력 계층에서 부터 0.4 , 0.3 이라는 값으로 오차 * 가중치가 넘어왔다면 이 둘의 합으로 또다른 오차가 노드에 생긴것이다.\
다시 이 오차를 앞 계층의 가중치와 곱해서 다시 앞 계층의 노드로 넘기고 계속해서 입력 계층까지 계산하면 된다.\
이 오차 역시도 행렬을 이용해서 아주 쉽게 계산이 가능하다 행렬의 강력함!!

### 경사 하강법 - 함수의 최저점을 구하기 위해 좋은 접근 방법.
경사 하강법을 이용하면 함수의 처저점을 구하기 쉬우며 이는 오차의 최저점을 찾기 쉽다는것을 의미한다.\
만약 현재 x 값에서 그래프의 기울기가 양수라면 x의 값을 감소시켜야 기울기가 줄어들고,\
기울기가 음수라면 x의 값을 증가시켜야 기울기가 증가한다. 이를 이용하여서 기울기의 정도를 보며\
계속해서 x의 값을 바꿔나가다 보면 최저점에 도달하지 않을까 하는 방법이다.\
하지만 여기서 오류는 x,y축으로 이루어진 데이터가아니라 수많은 매개변수의 경우에 그래프에서\
구한 최저점이 최저점이 아닐 가능성이 높다. 따라서 우리는 계속해서 다른 출발점에서 시작하여서 이를 반복,\
그 중 가장 최저점을 찾도록 접근해야한다.

## 핵심
 1. 신경망의 오차는 가중치의 함수이다.
 2. 신경망을 개선한다는 것은 가중치의 변화를 통해 오차를 줄인다는 뜻이다.
 3. 최적의 가중치는 경사 하강법을 이용하여 구한다.
 4. 오차 기울기는 미분을 이용해서 계산한다.
 5. 가중치는 0을 가지면 안되고, 같은 노드들이 서로 다른 가중치를 가져야한다. 임의의 작은 값을 가져야 함.
 6. 입력 값은 작은 값으로 하되, 0으로 설정해서는 안된다. 0.01 ~ 0.99 또는 -1 ~ 1이 일반적인 범위.
 7. 출력 값은 활성화 함수가 생성할 수 있는 범위 내에 있어야 한다. 즉, 시그모이드 함수에서 가능한 0.01 ~ 0.99가 적당하다.

