# 강화학습이란

![image](https://user-images.githubusercontent.com/37290818/149907431-fe22c060-5119-497d-b575-834bc883a4d4.png)


지구의 모든 생물이 공유하며 모든 지능적인 행동의 기반이 되는 관계가 바로 상호 관계. \
이런 상호 관계를 포착하여 공식적인 모델로 만든것이 바로 강화학습이다.


강화학습은 지도학습과 다르게 입력값과 미리 획득한 출력값의 쌍이 필요가 없다.\
대신 강화학습 알고리즘에서는 에이전트가 관찰을 통해 환경으로부터 얻는 보상과 취할 수 있는 액션, 즉 보상과 액션의 정확한 쌍을 학습할 수 있게 해야함.\
우리는 어떤 주어진 환경에서 에이전트에게 '이번에는 이런 액션을 하면 돼!'라고 알려줄 수 있는 일련의 '진짜' 정확한 액션에 대한 정보를 가지고 있지 않다.\
따라서 에이전트는 어떤 액션을 취해야만 시간의 흐름에 따라 가장 큰 보상을 가져올지를 스스로 학습해야함.\
이러한 개념을 다른 말로 할인된 기대 보상(discounted expected reward)의 최적화라고 표현할 수 있으며, 이것이 바로 우리가 에이전트에게 원하는 것.

## 정책(policy)

정책은 주어진 환경의 어떤 상황에서 어떤 에이전트가 취하게 되는 일련의 액션을 기술합니다.\
에이전트가 주어진 환경 내에서 (시간이 경과 후 최종적으로) 최대의 보상을 얻는 정책을 최적의 정책으로 간주하게 됩니다.

## 마르코프 프로세스

![image](https://user-images.githubusercontent.com/37290818/149907612-2bab59cb-3d8e-4684-9385-df6ba973b385.png)


미리 정의된 어떤 확률 분포를 따라서 정해진 시간 간격으로 상태와 상태 사이를 이동해 다니는 과정(Process)

 => 어떤 상태에 도착하면 다음상태가 어디가 될지 정의된 확률에 따라 정해지게 된다

하나의 상태에서 다음 상태로 이동을 가리키는 화살표는 확률에 따라 여러 개가 나타날 수 있으며 각 화살표로 이동확률의 합은 100% 이다

위의 그림에서 Ps0s1은 0.6 으로 s0에서 s1의 상태로 갈 확률을 나타낸다.

전이확률을 조건부 확률식으로 풀어서 표현하면 다음과 같다

![image](https://user-images.githubusercontent.com/37290818/149907893-ca81e469-1e7f-433f-b73c-04eef7c1838a.png)
![image](https://user-images.githubusercontent.com/37290818/149907933-65a4c476-59af-4277-9726-d59f9038fbe0.png)


## 마르코프 성질

![image](https://user-images.githubusercontent.com/37290818/149908226-350729d4-8a62-4f37-8d08-894c57ef6633.png)

"미래는 오로지 현재에 의해 결정된다"

마르코프한 상태의 예시 :

체스 , 바둑처럼 현재 상황에서 두어야하는 최선의 수에 과거의 정보가 영향을 주지 않고 항상 같다.\
즉, 어느 시점 t에 사진을 찍어 사진만 보여주고 수를 두라고 했을 때 손쉽게 최선의 수를 둘 수 있다.

마르코프 하지 않은 상태의 예시 :

운전 - 현재 자동자 주변 상황을 사진으로 찍어놓고 지금 상태에서 다음에 브레이크를 밟아야 할지 엑셀을 밟아야 할지 판단할 수 없다

 

따라서 단순히 이런 방식으로 고려한다면 운전은 마르코프 성질을 따르지 않는 프로세스 라고 할 수 있다.

하지만, 만일 1초 마다 자동자 주변상황을 찍어서 과거 사진 10장을 (즉 10초 전 상황) 하나의 상태로 묶어서 제공된다면 다음 상황을 고려할 수 있게 된다.

(영국 딥마인드사에서는 비디오 게임 학습시킬 때, 시점 t 에서의 이미지와 함께 t-1, t-2, t-3 의 과거 이미지를 엮어서 하나의 상태로 제공하였다. 이렇게 함으로 해당 프로세스를 조금이라도 더 마르코프 성질을 따르도록 하기 위함이다)

 

어떤 현상을 마르코프 프로세스로 모델링 하려면 상태가 마르코프 해야 하며, 단일 상태 정보만으로도 정보가 충분하도록 상태를 잘 구성해야 한다


## 마르코프 리워드 프로세스


마르코프 프로세스에 보상의 개념이 추가된 프로세스이다.

 ![image](https://user-images.githubusercontent.com/37290818/149913596-bb154e9c-602f-4ced-b6fa-1d5f07fd46f9.png)

마르코프 프로세스는 상태의 집합 S와 전이 확률 행렬 P로 정의 되었는데, MRP를 정의하기 위해서는 R과 r(감마)라는 2가지 요소가 추가로 필요하다. (R : 보상함수, r : 감마)

![image](https://user-images.githubusercontent.com/37290818/149913712-fafd38bc-653b-4761-8e29-e1fea0a77ebe.png)

### 보상함수 R

R은 어떤 상태 s에 도착할 때 받는 보상을 의미한다

![image](https://user-images.githubusercontent.com/37290818/149913754-04272dca-e134-479d-af60-85eda25f06cd.png)


기댓값이 나온 이유는 특정 상태에 도달했을 때 받는 보상이 매번 조금씩 다를 수 있기 때문이다.

예를 들어, 어떤 상태에 도달하면 보상을 줄 때 동전을 던져 앞면이 나오면 500원, 뒷면이 나오면 받지 못한다고 하면 실제 보상은 매번 달라질 것이다. 이런 경우 기댓값을 적용하면 보상은 250원이 된다. 

거꾸로 생각해보면 어떤 값을 표현할 때 기댓값을 사용했다는 의미는 그 값이 항상 나오는 고정된 값이 아니라는 의미를 내포하고 있는 것이다.


![image](https://user-images.githubusercontent.com/37290818/149913910-23ffb62e-7b37-4ad4-9c1f-30f5a49c95fd.png)


이와 같은 하나의 여정을 강화학습에서는 에피소드(episode) 라고 한다.

이런 표기법을 이용하여 바로 리턴(return) G_t를 정의할 수 있다. 리턴이란 t시점부터 미래에 받을 감쇠된 보상의 합을 말한다.

### 감마는 왜 필요한가?

감마를 1에 가까운 값을 사용하게 되면 미래 보상값이 리턴에 거의 그대로 영향을 주게되며 이것은 먼 미래도 중요시 생각하는 장기적인 시야를 갖도록 하는 모델이 되도록 할 것이다.

반대로 감마를 0에 가까운 값을 사용하게 되면 먼 미래 보상값은 0에 가깝게 작아지게 되어 근시안적인 모델이 되도록 할 것이다.

![image](https://user-images.githubusercontent.com/37290818/149914193-5719fb3e-16e7-48dc-a77e-a04adb9cbbd9.png)


따라서 r의 크기를 통해 미래에 얻게 될 보상에 비해 현재 얻는 보상에 가중치를 줄 수 있다.

에이전트의 목적은 지금부터 미래에 받을 보상의 합인 G_t를 최대화 하는 것이다.

r의 필요성에 대한 3가지 관점

수학적 편리성

r를 1보다 작게 해줌으로써 리턴 G_t가 무한의 값을 가지는 것을 방지할 수 있다.

사람의 선호 반영

사람은 기본적으로 당장 벌어지는 눈앞의 보상을 더 선호한다는 것을 알 수 있다. 이러한 이유에서 에이전트를 학습하는데 있어서 감마의 개념을 도입한다.

미래에 대한 불확실성 반영

현재와 미래 사이에는 다양한 확률적 요소들이 있고 이로 인해 당장 느끼는 가치에 비해 미래에 느끼는 가치가 달라질 수 있다. 그렇기 때문에 미래의 가치에는 불확실성을 반영하고자 감쇠를 해준다.


### MRP에서 각 상태의 밸류 평가하기

MRP에서 눈을 감고 있는 상태인 S2의 밸류를 숫자 하나로 딱 평가하고 싶으면 어떻게 해야할까?

어떤 상태의 가치를 평가할 때 과거보다 미래를 고려하여 평가하게 된다.

MRP에서도 특정 시점의 상태에 대한 가치 밸류를 계산하려면 해당 시점 기준으로 미래에 받을 보상들의 합이 높으면 그 상태 밸류가 높다고 생각할 수 있다.

여기서 이야기한 미래에 받을 보상의 합이 바로 리턴값 (G_t) 이다.

 

문제는 리턴값이 매번 바뀐다는 점이다.

확률적 요소에 의해 다음 상태가 정해지므로  t 시점 기준으로 다음 도달 상태들이 매번 달라질 수 있다. 결국 그에 대한 보상이 달라지므로 보상의 합인 리턴값이 매번 바뀐다는 것이다.

이때 사용되는 개념이 기댓값 이다.

따라서 s2의 밸류를 평가할 때는 미래의 리턴에 대한 기댓값을 사용하는 것입니다.

### 샘플링

시작 상태 S0에서 Sr까지 가는 하나의 여정을 에피소드라고 하였다.

그런데 하나의 에피소드 안에서 방문하는 상태들은 매번 다르며 그에 따라 리턴도 달라진다.

이를 매번 에피소드가 어떻게 샘플링 되느냐에 따라 리턴이 달라진다고 표현한다.

![image](https://user-images.githubusercontent.com/37290818/149915740-707f4093-caad-4324-b61e-10bd896e3950.png)

 

### 상태가치함수(State Value Function)

상태가치함수는 임의의 상태 S를 인풋으로 넣으면 그 상태의 밸류를 아웃풋으로 출력하는 함수이다.
 

## MDP(Markov decision process - 마르코프 결정 과정)
주어진 환경에서의 보상과 상태 전이를 제공하는 데 그치지 않고\
보상 역시 환경의 상태와 에이전트가 자신의 상태에서 취하는 액션에 의해 좌우 됩니다.\
이러한 역학 역시 시간의 영향을 받으며 지연 될 수 있음.


참고한 블로그 - https://myetc.tistory.com/33?category=969650 , https://velog.io/@suminwooo/%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-2
