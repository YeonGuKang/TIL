# 강화학습이란

![image](https://user-images.githubusercontent.com/37290818/149907431-fe22c060-5119-497d-b575-834bc883a4d4.png)


지구의 모든 생물이 공유하며 모든 지능적인 행동의 기반이 되는 관계가 바로 상호 관계. \
이런 상호 관계를 포착하여 공식적인 모델로 만든것이 바로 강화학습이다.


강화학습은 지도학습과 다르게 입력값과 미리 획득한 출력값의 쌍이 필요가 없다.\
대신 강화학습 알고리즘에서는 에이전트가 관찰을 통해 환경으로부터 얻는 보상과 취할 수 있는 액션, 즉 보상과 액션의 정확한 쌍을 학습할 수 있게 해야함.\
우리는 어떤 주어진 환경에서 에이전트에게 '이번에는 이런 액션을 하면 돼!'라고 알려줄 수 있는 일련의 '진짜' 정확한 액션에 대한 정보를 가지고 있지 않다.\
따라서 에이전트는 어떤 액션을 취해야만 시간의 흐름에 따라 가장 큰 보상을 가져올지를 스스로 학습해야함.\
이러한 개념을 다른 말로 할인된 기대 보상(discounted expected reward)의 최적화라고 표현할 수 있으며, 이것이 바로 우리가 에이전트에게 원하는 것.

## 정책(policy)

정책은 주어진 환경의 어떤 상황에서 어떤 에이전트가 취하게 되는 일련의 액션을 기술합니다.\
에이전트가 주어진 환경 내에서 (시간이 경과 후 최종적으로) 최대의 보상을 얻는 정책을 최적의 정책으로 간주하게 됩니다.

## 마르코프 프로세스

![image](https://user-images.githubusercontent.com/37290818/149907612-2bab59cb-3d8e-4684-9385-df6ba973b385.png)


미리 정의된 어떤 확률 분포를 따라서 정해진 시간 간격으로 상태와 상태 사이를 이동해 다니는 과정(Process)

 => 어떤 상태에 도착하면 다음상태가 어디가 될지 정의된 확률에 따라 정해지게 된다

하나의 상태에서 다음 상태로 이동을 가리키는 화살표는 확률에 따라 여러 개가 나타날 수 있으며 각 화살표로 이동확률의 합은 100% 이다

위의 그림에서 Ps0s1은 0.6 으로 s0에서 s1의 상태로 갈 확률을 나타낸다.

전이확률을 조건부 확률식으로 풀어서 표현하면 다음과 같다

![image](https://user-images.githubusercontent.com/37290818/149907893-ca81e469-1e7f-433f-b73c-04eef7c1838a.png)
![image](https://user-images.githubusercontent.com/37290818/149907933-65a4c476-59af-4277-9726-d59f9038fbe0.png)


## 마르코프 성질

![image](https://user-images.githubusercontent.com/37290818/149908226-350729d4-8a62-4f37-8d08-894c57ef6633.png)

"미래는 오로지 현재에 의해 결정된다"

마르코프한 상태의 예시 :

체스 , 바둑처럼 현재 상황에서 두어야하는 최선의 수에 과거의 정보가 영향을 주지 않고 항상 같다.\
즉, 어느 시점 t에 사진을 찍어 사진만 보여주고 수를 두라고 했을 때 손쉽게 최선의 수를 둘 수 있다.

마르코프 하지 않은 상태의 예시 :

운전 - 현재 자동자 주변 상황을 사진으로 찍어놓고 지금 상태에서 다음에 브레이크를 밟아야 할지 엑셀을 밟아야 할지 판단할 수 없다

 

따라서 단순히 이런 방식으로 고려한다면 운전은 마르코프 성질을 따르지 않는 프로세스 라고 할 수 있다.

하지만, 만일 1초 마다 자동자 주변상황을 찍어서 과거 사진 10장을 (즉 10초 전 상황) 하나의 상태로 묶어서 제공된다면 다음 상황을 고려할 수 있게 된다.

(영국 딥마인드사에서는 비디오 게임 학습시킬 때, 시점 t 에서의 이미지와 함께 t-1, t-2, t-3 의 과거 이미지를 엮어서 하나의 상태로 제공하였다. 이렇게 함으로 해당 프로세스를 조금이라도 더 마르코프 성질을 따르도록 하기 위함이다)

 

어떤 현상을 마르코프 프로세스로 모델링 하려면 상태가 마르코프 해야 하며, 단일 상태 정보만으로도 정보가 충분하도록 상태를 잘 구성해야 한다


## 마르코프 리워드 프로세스


마르코프 프로세스에 보상의 개념이 추가된 프로세스이다.

 ![image](https://user-images.githubusercontent.com/37290818/149913596-bb154e9c-602f-4ced-b6fa-1d5f07fd46f9.png)

마르코프 프로세스는 상태의 집합 S와 전이 확률 행렬 P로 정의 되었는데, MRP를 정의하기 위해서는 R과 r(감마)라는 2가지 요소가 추가로 필요하다. (R : 보상함수, r : 감마)

![image](https://user-images.githubusercontent.com/37290818/149913712-fafd38bc-653b-4761-8e29-e1fea0a77ebe.png)

### 보상함수 R

R은 어떤 상태 s에 도착할 때 받는 보상을 의미한다

![image](https://user-images.githubusercontent.com/37290818/149913754-04272dca-e134-479d-af60-85eda25f06cd.png)


기댓값이 나온 이유는 특정 상태에 도달했을 때 받는 보상이 매번 조금씩 다를 수 있기 때문이다.

예를 들어, 어떤 상태에 도달하면 보상을 줄 때 동전을 던져 앞면이 나오면 500원, 뒷면이 나오면 받지 못한다고 하면 실제 보상은 매번 달라질 것이다. 이런 경우 기댓값을 적용하면 보상은 250원이 된다. 

거꾸로 생각해보면 어떤 값을 표현할 때 기댓값을 사용했다는 의미는 그 값이 항상 나오는 고정된 값이 아니라는 의미를 내포하고 있는 것이다.


![image](https://user-images.githubusercontent.com/37290818/149913910-23ffb62e-7b37-4ad4-9c1f-30f5a49c95fd.png)


이와 같은 하나의 여정을 강화학습에서는 에피소드(episode) 라고 한다.

이런 표기법을 이용하여 바로 리턴(return) G_t를 정의할 수 있다. 리턴이란 t시점부터 미래에 받을 감쇠된 보상의 합을 말한다.

### 감마는 왜 필요한가?

감마를 1에 가까운 값을 사용하게 되면 미래 보상값이 리턴에 거의 그대로 영향을 주게되며 이것은 먼 미래도 중요시 생각하는 장기적인 시야를 갖도록 하는 모델이 되도록 할 것이다.

반대로 감마를 0에 가까운 값을 사용하게 되면 먼 미래 보상값은 0에 가깝게 작아지게 되어 근시안적인 모델이 되도록 할 것이다.

![image](https://user-images.githubusercontent.com/37290818/149914193-5719fb3e-16e7-48dc-a77e-a04adb9cbbd9.png)


따라서 r의 크기를 통해 미래에 얻게 될 보상에 비해 현재 얻는 보상에 가중치를 줄 수 있다.

에이전트의 목적은 지금부터 미래에 받을 보상의 합인 G_t를 최대화 하는 것이다.

r의 필요성에 대한 3가지 관점

수학적 편리성

r를 1보다 작게 해줌으로써 리턴 G_t가 무한의 값을 가지는 것을 방지할 수 있다.

사람의 선호 반영

사람은 기본적으로 당장 벌어지는 눈앞의 보상을 더 선호한다는 것을 알 수 있다. 이러한 이유에서 에이전트를 학습하는데 있어서 감마의 개념을 도입한다.

미래에 대한 불확실성 반영

현재와 미래 사이에는 다양한 확률적 요소들이 있고 이로 인해 당장 느끼는 가치에 비해 미래에 느끼는 가치가 달라질 수 있다. 그렇기 때문에 미래의 가치에는 불확실성을 반영하고자 감쇠를 해준다.


### MRP에서 각 상태의 밸류 평가하기

MRP에서 눈을 감고 있는 상태인 S2의 밸류를 숫자 하나로 딱 평가하고 싶으면 어떻게 해야할까?

어떤 상태의 가치를 평가할 때 과거보다 미래를 고려하여 평가하게 된다.

MRP에서도 특정 시점의 상태에 대한 가치 밸류를 계산하려면 해당 시점 기준으로 미래에 받을 보상들의 합이 높으면 그 상태 밸류가 높다고 생각할 수 있다.

여기서 이야기한 미래에 받을 보상의 합이 바로 리턴값 (G_t) 이다.

 

문제는 리턴값이 매번 바뀐다는 점이다.

확률적 요소에 의해 다음 상태가 정해지므로  t 시점 기준으로 다음 도달 상태들이 매번 달라질 수 있다. 결국 그에 대한 보상이 달라지므로 보상의 합인 리턴값이 매번 바뀐다는 것이다.

이때 사용되는 개념이 기댓값 이다.

따라서 s2의 밸류를 평가할 때는 미래의 리턴에 대한 기댓값을 사용하는 것입니다.

### 샘플링

시작 상태 S0에서 Sr까지 가는 하나의 여정을 에피소드라고 하였다.

그런데 하나의 에피소드 안에서 방문하는 상태들은 매번 다르며 그에 따라 리턴도 달라진다.

이를 매번 에피소드가 어떻게 샘플링 되느냐에 따라 리턴이 달라진다고 표현한다.

![image](https://user-images.githubusercontent.com/37290818/149915740-707f4093-caad-4324-b61e-10bd896e3950.png)

 

### 상태가치함수(State Value Function)

상태가치함수는 임의의 상태 S를 인풋으로 넣으면 그 상태의 밸류를 아웃풋으로 출력하는 함수이다.

![image](https://user-images.githubusercontent.com/37290818/149915902-2aa80681-1c05-463e-be73-5d4d6613faa4.png)

(상태 s로부터 시작하여 얻는 리턴의 기댓값)

조건부로 붙는 의 의미는 시점 t에서 상태 s부터 시작하여 에피소드가 끝날 때까지의 리턴을 계산하라는 뜻

같은 상태에서 출발하여도 주어진 확률분포때문에 매번 실행할 때마다 에피소드가 달라지면서 리턴값이 달라지므로 

상태가치함수의 인풋으로 동일한 S를 입력하여도 결과 리턴값이 다르게 나오게 된다.

이런 경우 위에서 언급한 기댓값 성질을 이용한다.

![image](https://user-images.githubusercontent.com/37290818/149916185-717edb50-8c37-443f-a48c-0c5e8fe81c09.png)


여기서  St상태의 가치를 계산하는 방법으로 리턴의 기댓값을 사용한다는 의미는 무엇일까?

St 상태부터 종료상태까지 해당 프로세스를 상당히 여러번 실행시켜 다양한 에피소드 샘플들을 얻어 각 샘플 에피소드의 리턴값을 합산 후 평균을 냄으로 기댓값으로 활용할 수 있다는 의미이다.

위 그림을 예를 들면 4.3, 4.6, 6.2, 5.4의 평균인 5.1이 V(s0)가 된다.

하지만 가능한 에피소드가 무한히 많기 때문에 이런 접근법은 현실적으로 불가능하다. 

현재까지 정리하자면 현재 시점부터 받을 보상의 합이 리턴이을 배웠고, 같은 상태에서 출발하여도 에피소드마다\
리턴이 달라지므로 주어진 상태의 가치를 리턴의 기댓값을 통해 정의할 수 있다는 것을 배웠다.\
그러면 이제 우리의 목표인 MDP로 넘어가보자.

 

## MDP(Markov decision process - 마르코프 결정 과정)

MRP를 복기해보면 상태 변화가 자동으로 이루어졌다. 다시 말하면 다음 상태의 분포가 미리 정해져 있었다.\
여기에는 행동이라는 주체랄 것이 없었기 때문에, 아이가 깨고 잠드는 예시 또한 미리 정해진 확률에 따라 정해지는 과정이였다.\
따라서 MP나 MRP만 가지고는 순차적 의사결정 문제를 모델링 할 수 없다.\
순차적 의사결정에서는 의사결정(decision)이 핵심이기 때문이다.\

이런 이유 때문에 의사결정 관한 부분이 모델에 들어간 마르코프 결정 프로세스(MDP)가 등장한 것이다.

MDP는 MRP에 에이전트가 더해진 것이다.\
에이전트는 각 상황마다 액션(행동)을 취한다.\
해당 액션에 의해 상태가 변하고 그에 따른 보상을 받는다.\
이 때문에 MDP를 정의하기 위해서는 하나의 요소가 추가되니, 그것은 바로 액션의 집합 A이다.\
A가 추가 되면서 기존의 전이 확률 행력 P나 보상함수 R의 정의도 MRP에서의 정의와 약간씩 달라지게 된다.

![image](https://user-images.githubusercontent.com/37290818/150126317-7129a91b-b5fb-40c1-a41f-67956a75f8af.png)

### 새로추가된 액션의 집합 A

해당 프로세스에서 취할 수 있는 모든 액션을 모아놓은 집합이다.

예를 들면, 포트폴리오를 운영하는 펀드매니저를 모델링한다면 펀드매니저가 취할 수 있는 모든 행동은 "매수하기", "매도하기", "관망하기" 이렇게 3가지로 정리할 수 있으며 따라서 액션 집합 A는 
 
![image](https://user-images.githubusercontent.com/37290818/150126472-3e983800-168a-4867-bc57-e7ff01193587.png)

에이전트는 스텝마다 위 액션의 집합 중 하나를 선택하여 액션을 취하며 그에 따라 다음 상태가 달라진다.

주어진 환경에서의 보상과 상태 전이를 제공하는 데 그치지 않고\
보상 역시 환경의 상태와 에이전트가 자신의 상태에서 취하는 액션에 의해 좌우 됩니다.\
이러한 역학 역시 시간의 영향을 받으며 지연 될 수 있음.

### 새로운 전이 확률 행렬 P

![image](https://user-images.githubusercontent.com/37290818/150126665-dfe8be4b-bea7-493f-bb59-633dff1ee985.png)

위 식의 의미 : 현재상태가 s 에서 에이전트가 액션 a를 선택했을 때 다음상태가 s'이 될 확률

즉, 에이전트가 같은 행동을 하더라도 그 결과로 매번 같은 상태로 변하지 않으며 다음상태 변화는 정해놓은 확률 분포를 따른다는 의미이다.

다르게 말하면 상태 s에서 액션 a를 선택했을 때 도달하게 되는 상태가 결정론적이 아니라는 점이다.

같은 상태 s에서 액션 a를 선택해도 매번 다른 상태에 도착할 수 있다.

따라서 이런 확률적 요소를 표현하기 위해 전이 확률로 표현하는 것이다.

전이확률의 엄밀한 정의는 다음과 같다

![image](https://user-images.githubusercontent.com/37290818/150126698-dc15fa87-0fde-4dc8-a9b6-73845cf9e27b.png)

이를 해석하면 "현재 상태 s에서, 액션 a를 했을 떄"가 조건으로 붙는 것이다.

### 보상 함수 R

MRP에서는 상태에 의해 보상이 정해졌다. 하지만 MDP에서는 액션이 추가되었기 때문에 현재 상태 s에서 어떤 액션을 선택하느냐에 따라 받는 보상이 달라진다.

MRP 에서는 상태 s 에서 바로 보상이 정해졌지만 MDP에서는 상태 s에서 에이전트가 액션 a를 선택해야 보상을 정할 수 있게 된다.\
물론 여기서 보상도 확률적으로 접근되어 액션 a 를 선택해도 매번 보상은 달라지므로 보상의 기댓값을 사용해야 한다\

MDP 에서 보상함수 R은 다음과 같다

![image](https://user-images.githubusercontent.com/37290818/150127213-8ae66e70-625b-4b31-8724-251593c0d31a.png)


### 다시 보는 아이재우기 MDP

![image](https://user-images.githubusercontent.com/37290818/150128020-31258139-177a-4da7-b992-17f06c00747b.png)


아이가 잠드는 상황에서 어머니라는 에이전트가 개입되었다.\
어머니가 선택할 수 있는 액션은 자장가를 불러주는 액션 a0와, 함께 놀아주는 액션 a1 이렇게 2가지가 존재한다.\

여기서 눈여겨 볼 점은 아이가 눈을 감은 상태인 s2에서 아이에게 놀아주는 액션을 선택하면 아이의 다음 상태는\
그날 아이의 상태에 따라 s0가 될 수도 있고, s1이 될 수 도 있다는 것이다.\
이때의 전이 확률을 수식적으로 표현하면 아래와 같다.

![image](https://user-images.githubusercontent.com/37290818/150128057-4094773d-12c5-4f4b-9f8b-ffa7b8bc74a7.png)

위 MDP 예시에서는 어머니가 하는 선택에 따라 보상이 달라지는데, 어머니는 a0만을 선택하면 최적의 보상을 얻을 수 있다는 것을\
손 쉽게 알 수 있다. 하지만 MDP가 복잡해지면 최적행동을 찾는 것이 그렇게 쉽지 않다.

![image](https://user-images.githubusercontent.com/37290818/150128361-fffda111-27e8-404f-81c9-a26a06ec3657.png)

(그림 2-11)

위 예시를 보면 전이 확률이 조금 달라졌고, 상태 s0에서 선택할 수 있는 액션이 1개 늘어났을 뿐인데 이전 MDP에 비해 그림이 많이 복잡해졌다.\
여기서는 간단하게 누적 보상을 최대화하기 위해 상황별로 어떤 선택을 해야 할지 쉽게 감이 오지 않는다.\
위 MDP는 상태의 개수가 5개, 액션의 개수가 많아야 3개이지만 실세계에서 MDP는 상태의 개수가 수백억 개가 넘을 정도로 무수히 많고, 액션의 개수도 훨씬 많다.\
이처럼 복잡한 MDP에서 결국 우리가 찾고자 하는 것은 각 상태 s에 따라 어떤 액션 a를 선택해야 보상의 합을 최대로 할 수 있는가입니다.\

이것을 위에서는 전략이라고 표현했지만 앞으로는 "정책"이라고 표현합니다.

### 정책 / 정책 함수

정책함수는 전체 보상의 합이 최대가 되기 위해서 각 상태에서 어떤 액션을 선택할 지 정해주는 함수이다.

정책함수는 보통 그리스 문자 파이를 사용해서 표기하며, 이는 원주율과는 아무런 관계가 없다.

정책 함수를 확률을 이용하여 정의하면 아래와 같다.

![image](https://user-images.githubusercontent.com/37290818/150132207-38dfd0b0-04b4-4d66-867a-8b7000219957.png)

예를 들어 이전 그림 2-11을 보면 MDP 속 상태 s0에서 선택할 수 있는 액션은 a0,a1,a2 이렇게 3가지입니다.\
그리고 이 각각에 대해 얼마큼의 확률을 부여할지를 정책함수가 결정합니다. 예를 들면 아래와 같습니다.

![image](https://user-images.githubusercontent.com/37290818/150132428-0bde592e-b900-4164-812b-da52a3df3470.png)

한 가지 주의할 점은 정책 함수는 에이전트 안에 존재한다는 점이다.\
그림 2-11을 보면 정책에 대한 내용, 즉 상태별 액션을 얼만큼의 확률로 고를지에 대한 내용은 전혀 없다.(a0,a1,a2를 고를 확률에 대한 내용이 X)\
그림 2-11은 환경에 대한 그림이기 때문에 환경은 변하지 않지만 에이전트는 자신의 정책을 언제든 수정 할 수 있다.\
더 큰 보상을 얻기 위해 계속해서 정책을 교정해 나가는 것이 곧 강화학습이다!!!!\

다시 한번 강조하자면 그림 2-11은 환경에 대한 것이고, 정책은 에이전트 안에 있다는 점이다!!

### 




참고한 블로그 - https://myetc.tistory.com/33?category=969650 , https://velog.io/@suminwooo/%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-2
