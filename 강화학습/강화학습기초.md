# 강화학습이란

![image](https://user-images.githubusercontent.com/37290818/149907431-fe22c060-5119-497d-b575-834bc883a4d4.png)


지구의 모든 생물이 공유하며 모든 지능적인 행동의 기반이 되는 관계가 바로 상호 관계. \
이런 상호 관계를 포착하여 공식적인 모델로 만든것이 바로 강화학습이다.


강화학습은 지도학습과 다르게 입력값과 미리 획득한 출력값의 쌍이 필요가 없다.\
대신 강화학습 알고리즘에서는 에이전트가 관찰을 통해 환경으로부터 얻는 보상과 취할 수 있는 액션, 즉 보상과 액션의 정확한 쌍을 학습할 수 있게 해야함.\
우리는 어떤 주어진 환경에서 에이전트에게 '이번에는 이런 액션을 하면 돼!'라고 알려줄 수 있는 일련의 '진짜' 정확한 액션에 대한 정보를 가지고 있지 않다.\
따라서 에이전트는 어떤 액션을 취해야만 시간의 흐름에 따라 가장 큰 보상을 가져올지를 스스로 학습해야함.\
이러한 개념을 다른 말로 할인된 기대 보상(discounted expected reward)의 최적화라고 표현할 수 있으며, 이것이 바로 우리가 에이전트에게 원하는 것.

## 정책(policy)

정책은 주어진 환경의 어떤 상황에서 어떤 에이전트가 취하게 되는 일련의 액션을 기술합니다.\
에이전트가 주어진 환경 내에서 (시간이 경과 후 최종적으로) 최대의 보상을 얻는 정책을 최적의 정책으로 간주하게 됩니다.

## 마르코프 프로세스

![image](https://user-images.githubusercontent.com/37290818/149907612-2bab59cb-3d8e-4684-9385-df6ba973b385.png)


미리 정의된 어떤 확률 분포를 따라서 정해진 시간 간격으로 상태와 상태 사이를 이동해 다니는 과정(Process)

 => 어떤 상태에 도착하면 다음상태가 어디가 될지 정의된 확률에 따라 정해지게 된다

하나의 상태에서 다음 상태로 이동을 가리키는 화살표는 확률에 따라 여러 개가 나타날 수 있으며 각 화살표로 이동확률의 합은 100% 이다

위의 그림에서 Ps0s1은 0.6 으로 s0에서 s1의 상태로 갈 확률을 나타낸다.

전이확률을 조건부 확률식으로 풀어서 표현하면 다음과 같다

![image](https://user-images.githubusercontent.com/37290818/149907893-ca81e469-1e7f-433f-b73c-04eef7c1838a.png)
![image](https://user-images.githubusercontent.com/37290818/149907933-65a4c476-59af-4277-9726-d59f9038fbe0.png)


## 마르코프 성질

![image](https://user-images.githubusercontent.com/37290818/149908226-350729d4-8a62-4f37-8d08-894c57ef6633.png)
"미래는 오로지 현재에 의해 결정된다"

마르코프한 상태의 예시 :

체스 , 바둑처럼 현재 상황에서 두어야하는 최선의 수에 과거의 정보가 영향을 주지 않고 항상 같다.\
즉, 어느 시점 t에 사진을 찍어 사진만 보여주고 수를 두라고 했을 때 손쉽게 최선의 수를 둘 수 있다.

마르코프 하지 않은 상태의 예시 :

운전 - 현재 자동자 주변 상황을 사진으로 찍어놓고 지금 상태에서 다음에 브레이크를 밟아야 할지 엑셀을 밟아야 할지 판단할 수 없다

 

따라서 단순히 이런 방식으로 고려한다면 운전은 마르코프 성질을 따르지 않는 프로세스 라고 할 수 있다.

하지만, 만일 1초 마다 자동자 주변상황을 찍어서 과거 사진 10장을 (즉 10초 전 상황) 하나의 상태로 묶어서 제공된다면 다음 상황을 고려할 수 있게 된다.

(영국 딥마인드사에서는 비디오 게임 학습시킬 때, 시점 t 에서의 이미지와 함께 t-1, t-2, t-3 의 과거 이미지를 엮어서 하나의 상태로 제공하였다. 이렇게 함으로 해당 프로세스를 조금이라도 더 마르코프 성질을 따르도록 하기 위함이다)

 

어떤 현상을 마르코프 프로세스로 모델링 하려면 상태가 마르코프 해야 하며, 단일 상태 정보만으로도 정보가 충분하도록 상태를 잘 구성해야 한다


## 마르코프 리워드 프로세스


마르코프 프로세스에 보상의 개념이 추가된 프로세스이다.

 ![image](https://user-images.githubusercontent.com/37290818/149913596-bb154e9c-602f-4ced-b6fa-1d5f07fd46f9.png)

마르코프 프로세스는 상태의 집합 S와 전이 확률 행렬 P로 정의 되었는데, MRP를 정의하기 위해서는 R과 r(감마)라는 2가지 요소가 추가로 필요하다. (R : 보상함수, r : 감마)

![image](https://user-images.githubusercontent.com/37290818/149913712-fafd38bc-653b-4761-8e29-e1fea0a77ebe.png)

- 보상함수 R

R은 어떤 상태 s에 도착할 때 받는 보상을 의미한다

![image](https://user-images.githubusercontent.com/37290818/149913754-04272dca-e134-479d-af60-85eda25f06cd.png)


기댓값이 나온 이유는 특정 상태에 도달했을 때 받는 보상이 매번 조금씩 다를 수 있기 때문이다.

예를 들어, 어떤 상태에 도달하면 보상을 줄 때 동전을 던져 앞면이 나오면 500원, 뒷면이 나오면 받지 못한다고 하면 실제 보상은 매번 달라질 것이다. 이런 경우 기댓값을 적용하면 보상은 250원이 된다. 

거꾸로 생각해보면 어떤 값을 표현할 때 기댓값을 사용했다는 의미는 그 값이 항상 나오는 고정된 값이 아니라는 의미를 내포하고 있는 것이다.


![image](https://user-images.githubusercontent.com/37290818/149913910-23ffb62e-7b37-4ad4-9c1f-30f5a49c95fd.png)

 

## MDP(Markov decision process - 마르코프 결정 과정)
주어진 환경에서의 보상과 상태 전이를 제공하는 데 그치지 않고\
보상 역시 환경의 상태와 에이전트가 자신의 상태에서 취하는 액션에 의해 좌우 됩니다.\
이러한 역학 역시 시간의 영향을 받으며 지연 될 수 있음.


참고한 블로그 - https://myetc.tistory.com/33?category=969650 , https://velog.io/@suminwooo/%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-2
