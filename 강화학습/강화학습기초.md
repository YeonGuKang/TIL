# 강화학습이란

![image](https://user-images.githubusercontent.com/37290818/149907431-fe22c060-5119-497d-b575-834bc883a4d4.png)


지구의 모든 생물이 공유하며 모든 지능적인 행동의 기반이 되는 관계가 바로 상호 관계. \
이런 상호 관계를 포착하여 공식적인 모델로 만든것이 바로 강화학습이다.


강화학습은 지도학습과 다르게 입력값과 미리 획득한 출력값의 쌍이 필요가 없다.\
대신 강화학습 알고리즘에서는 에이전트가 관찰을 통해 환경으로부터 얻는 보상과 취할 수 있는 액션, 즉 보상과 액션의 정확한 쌍을 학습할 수 있게 해야함.\
우리는 어떤 주어진 환경에서 에이전트에게 '이번에는 이런 액션을 하면 돼!'라고 알려줄 수 있는 일련의 '진짜' 정확한 액션에 대한 정보를 가지고 있지 않다.\
따라서 에이전트는 어떤 액션을 취해야만 시간의 흐름에 따라 가장 큰 보상을 가져올지를 스스로 학습해야함.\
이러한 개념을 다른 말로 할인된 기대 보상(discounted expected reward)의 최적화라고 표현할 수 있으며, 이것이 바로 우리가 에이전트에게 원하는 것.

## 정책(policy)

정책은 주어진 환경의 어떤 상황에서 어떤 에이전트가 취하게 되는 일련의 액션을 기술합니다.\
에이전트가 주어진 환경 내에서 (시간이 경과 후 최종적으로) 최대의 보상을 얻는 정책을 최적의 정책으로 간주하게 됩니다.

## 마르코프 프로세스

![image](https://user-images.githubusercontent.com/37290818/149907612-2bab59cb-3d8e-4684-9385-df6ba973b385.png)


미리 정의된 어떤 확률 분포를 따라서 정해진 시간 간격으로 상태와 상태 사이를 이동해 다니는 과정(Process)

 => 어떤 상태에 도착하면 다음상태가 어디가 될지 정의된 확률에 따라 정해지게 된다

하나의 상태에서 다음 상태로 이동을 가리키는 화살표는 확률에 따라 여러 개가 나타날 수 있으며 각 화살표로 이동확률의 합은 100% 이다

위의 그림에서 Ps0s1은 0.6 으로 s0에서 s1의 상태로 갈 확률을 나타낸다.

전이확률을 조건부 확률식으로 풀어서 표현하면 다음과 같다

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msub>
    <mi>P</mi>
    <mrow data-mjx-texclass="ORD">
      <mi>s</mi>
      <msup>
        <mi>s</mi>
        <mrow data-mjx-texclass="ORD">
          <msup>
            <mi></mi>
            <mo data-mjx-alternate="1">&#x2032;</mo>
          </msup>
        </mrow>
      </msup>
    </mrow>
  </msub>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="double-struck">P</mi>
  </mrow>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">[</mo>
    <msub>
      <mi>S</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <mo>=</mo>
    <msup>
      <mi>s</mi>
      <mrow data-mjx-texclass="ORD">
        <msup>
          <mi></mi>
          <mo data-mjx-alternate="1">&#x2032;</mo>
        </msup>
      </mrow>
    </msup>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">|</mo>
    <msub>
      <mi>S</mi>
      <mi>t</mi>
    </msub>
    <mo>=</mo>
    <mi>s</mi>
    <mo data-mjx-texclass="CLOSE">]</mo>
  </mrow>
</math>
 



## 마르코프 리워드 프로세스

## MDP(Markov decision process - 마르코프 결정 과정)
주어진 환경에서의 보상과 상태 전이를 제공하는 데 그치지 않고\
보상 역시 환경의 상태와 에이전트가 자신의 상태에서 취하는 액션에 의해 좌우 됩니다.\
이러한 역학 역시 시간의 영향을 받으며 지연 될 수 있음.
